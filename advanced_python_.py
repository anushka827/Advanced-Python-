# -*- coding: utf-8 -*-
"""advanced python .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/anushka827/Advanced-Python-/blob/main/advanced%20python%20.ipynb

**PART-1 process automation**

Q1. create a line that contain 1000 lines of random strings
"""

import random as r
import string as s

with open("file.txt","w") as f:
  for _ in range(1000):
    length=r.randint(5,20)
    letters=s.ascii_letters
    rand_string="".join(r.choices(letters,k=length))
    f.write(rand_string+'\n')
print("File file.txt created with 1000 lines of random strings.")

"""Q2.create lines that contain multiple lines of random strings and file size must be 5MB"""

import os
with open("string.txt","w") as f:
  size_mb=0
  while size_mb<=5:
    length=r.randint(5,20)
    letters=s.ascii_letters
    rand_string="".join(r.choices(letters,k=length))
    f.write(rand_string+'\n')
    size_bytes = os.path.getsize('/content/string.txt')
    size_mb = size_bytes / (1024 * 1024)

"""Q3. create 10 files that contains multiple lines of random strings and file of each file must be 5MB"""

size_mb=5*1024*1024
for i in range(0,10):
  filename=f"file+{i+1}.txt"
  with open(filename,'w') as f:
    curr_size=0
    while curr_size<=size_mb:
       length=r.randint(5,20)
       letters=s.ascii_letters
       rand_string="".join(r.choices(letters,k=length))
       f.write(rand_string+'\n')
       curr_size += len(rand_string.encode('utf-8'))

"""Q4. Create 5 files of size 1GB, 2GB, 3GB, 4GB and 5GB; file contains multiple lines of random strings."""

for i in range(1,6):
  filename=f'f_{i}.txt'
  size=i*1024*1024*1024
  with open(filename,'w') as f:
    curr_size=0
    while curr_size<=size:
      length=r.randint(5,20)
      letters=s.ascii_letters
      rand_string="".join(r.choices(letters,k=length))
      f.write(rand_string+'\n')
      curr_size += len(rand_string.encode('utf-8'))

"""Q5. Convert all the files of Q4 into upper case one by one."""

for i in range(1, 6):
    filename = f'f_{i}.txt'
    with open(filename, 'r') as f:
        lines = f.readlines()
    with open(filename, 'w') as f:
        for line in lines:
            f.write(line.upper())

"""Q6. Convert all the files of Q4 into upper case parallel using multi-threading."""

import threading

def convert_file_to_upper(filename):
    temp_filename = f"temp_{filename}"
    with open(filename, 'r') as infile, open(temp_filename, 'w') as outfile:
        for line in infile:
            outfile.write(line.upper())
    os.replace(temp_filename, filename)

threads = []
for i in range(1, 6):
    filename = f"f_{i}.txt"
    t = threading.Thread(target=convert_file_to_upper, args=(filename,))
    t.start()
    threads.append(t)

for t in threads:
    t.join()

print("All files converted to uppercase using multithreading.")

"""Q7. wap to download 10 images of cat using google lens"""

!pip install Google-Images-Search

from google_images_search import GoogleImagesSearch
import zipfile
import os
# you can provide API key and CX using arguments,
# or you can set environment variables: GCS_DEVELOPER_KEY, GCS_CX
gis = GoogleImagesSearch('AIzaSyCGyqf36D5k3QghaZLhAqb1R2OUtRFraF8' , '0d386b282da5209ea' , validate_images=True)
def search(keyword, imageNumber):
    _search_params = {
        'q': keyword,
        'num': imageNumber,
        # 'safe': 'medium',
        # 'fileType': 'jpg',
        # 'imgType': 'photo',
        # 'imgSize': 'MEDIUM',
        # 'imgDominantColor': 'brown',
        # 'rights': 'cc_publicdomain'
    }

    #path_to_dir: where the downloaded images must be stored
    gis.search(search_params=_search_params, path_to_dir='./images/')

##calling search function to download 10 motorbike images
search('cat',10)

"""Q8. wap to download 10 videos of machine learning from youtube"""

# Import necessary libraries
import os
from yt_dlp import YoutubeDL
from youtube_search import YoutubeSearch

print("--- Installing required libraries ---")
!pip install -q yt-dlp

!pip install -q youtube-search-python
print("--- Installation complete ---")

SEARCH_QUERY = "machine learning"
NUM_VIDEOS_TO_DOWNLOAD = 10

OUTPUT_DIR = "ml_videos"

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)
    print(f"Created directory: {OUTPUT_DIR}")


print(f"\n--- Searching for top {NUM_VIDEOS_TO_DOWNLOAD} '{SEARCH_QUERY}' videos on YouTube ---")
try:
    results = YoutubeSearch(SEARCH_QUERY, max_results=NUM_VIDEOS_TO_DOWNLOAD).to_dict()

    video_urls = [f"https://www.youtube.com/watch?v={video['id']}" for video in results['videos']]

    if not video_urls:
        print("No video URLs found for the given search query. Exiting.")
    else:
        print(f"Found {len(video_urls)} video URLs.")
        for i, url in enumerate(video_urls):
            print(f"{i+1}. {url}")

except Exception as e:
    print(f"An error occurred during video search: {e}")
    video_urls = []

if video_urls:
    print(f"\n--- Starting download of {len(video_urls)} videos ---")

    ydl_opts = {
        'format': 'bestvideo[ext=mp4]+bestaudio[ext=m4a]/best[ext=mp4]/best',
        'outtmpl': os.path.join(OUTPUT_DIR, '%(title)s.%(ext)s'),
        'noplaylist': True,
        'quiet': False,
        'no_warnings': True,
    }

    with YoutubeDL(ydl_opts) as ydl:
        for i, url in enumerate(video_urls):
            print(f"\n--- Downloading video {i+1}/{len(video_urls)}: {url} ---")
            try:
                ydl.download([url])
                print(f"Successfully downloaded: {url}")
            except Exception as e:
                print(f"Error downloading {url}: {e}")
else:
    print("No videos to download as no URLs were found or search failed.")

print("\n--- All requested operations complete ---")
print(f"You can find the downloaded videos in the '{OUTPUT_DIR}' folder in your Colab files sidebar.")

"""Q9. Convert all the videos of Q8 and convert it to audio."""

from moviepy.editor import VideoFileClip

folder = "ml_vids"
output_folder = "ml_audios"
os.makedirs(output_folder, exist_ok=True)

for filename in os.listdir(folder):
    if filename.lower().endswith(('.mp4', '.mkv', '.webm')):
        video_path = os.path.join(folder, filename)
        audio_filename = os.path.splitext(filename)[0] + ".mp3"
        audio_path = os.path.join(output_folder, audio_filename)

        try:
            print(f"Converting: {filename}")
            clip = VideoFileClip(video_path)
            clip.audio.write_audiofile(audio_path)
            clip.close()
        except Exception as e:
            print(f"Failed to convert {filename}: {e}")

"""**PART 2**

Q12.Create a random dataset of 100 rows and 30 columns. All the values are defined between [1,200].
"""

import pandas  as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df=pd.DataFrame(np.random.randint(1,200,size=(100,30)))

df.head()

"""(i) Replace all the values with NA in the dataset defined between [10, 60]. Print the count of number rows having missing values."""

df=df.mask((df>=10) & (df<=60),np.nan)
na_rows=df.isnull().sum()
print("Number of rows with missing values:")
print(na_rows)
print("Total number of rows having missing values : ",na_rows.sum())

"""(ii) Replace all the NA values with the average of the column value"""

df_filled = df.fillna(df.mean())
print(df_filled)
print("Number of rows with missing values:")
print(df_filled.isnull().sum())
print("Total number of rows having missing values : ",df_filled.isnull().sum().sum())

"""(iii) Find the Pearson correlation among all the columns and plot heat map. Also select those columns having correlation <=0.7."""

from textwrap import fill
corr_matrix= df.corr(method='pearson')
plt.figure(figsize=(10,10))
sns.heatmap(corr_matrix,annot=False,cmap='coolwarm',linewidth=0.5)
plt.title('Pearson Correlation Heatmap')
plt.tight_layout()
plt.show()
correlation_mask= corr_matrix.abs() > 0.7
np.fill_diagonal(correlation_mask.values,False)
low_corr_columns=correlation_mask.sum(axis=1)==0
filtered_df=df.loc[:,low_corr_columns]
print("Columns with correlation <=0.7 with all other columns:")
print(filtered_df.columns.tolist())

"""(iv) Normalize all the values in the dataset between 0 and 10"""

df_normalized=(df-df.min())/(df.max()-df.min())*10
print(df_normalized.head)

"""(v) Replace all the values in the dataset with 1 if value <=0.5 else with 0."""

cdf_binary=(df_normalized > 0.5 ).astype(int)       # df_normalized > 0.5 creates boolean mask where each value is True or False   astype(int) converts True to 1 and False to 0
print(df_binary.head())

"""Q13. Create a random dataset of 500 rows and 10 columns. Columns 1 to 4 are defined between [-10, 10]; Columns 5 to 8 are defined between [10, 20]; Columns 9 to 10 are defined between [-100, 100]. Apply following clustering algorithms; determine the optimal number of clusters and plot distance metric graph using each algorithm."""

cols_1_4 = pd.DataFrame(np.random.uniform(-10, 10, size=(500, 4)), columns=[f'Col{i}' for i in range(1, 5)])
cols_5_8 = pd.DataFrame(np.random.uniform(10, 20, size=(500, 4)), columns=[f'Col{i}' for i in range(5, 9)])
cols_9_10 = pd.DataFrame(np.random.uniform(-100, 100, size=(500, 2)), columns=[f'Col{i}' for i in range(9, 11)])

df = pd.concat([cols_1_4, cols_5_8, cols_9_10], axis=1)
df.head()

"""(i) K-Mean clustering"""

!pip uninstall -y wandb pycaret

!pip install pycaret

import numpy as np
import pandas as pd
from pycaret.clustering import *
import matplotlib.pyplot as plt
# ‚öôÔ∏è Step 4: Initialize PyCaret Clustering Setup
setup(data=df, normalize=True)

# ü§ñ Step 5: Create Initial KMeans Model
kmeans_model = create_model('kmeans')

# üìà Step 6: Plot Elbow Curve (to find optimal k)
plot_model(kmeans_model, plot='elbow')

# ‚ú® Step 7: Create Final KMeans Model with chosen k (e.g., 4)
final_kmeans = create_model('kmeans', num_clusters=4)

# üè∑Ô∏è Step 8: Assign Cluster Labels to Data
clustered_data = assign_model(final_kmeans)

# üñºÔ∏è Step 9: Visualize Clusters in 2D (via PCA)
plot_model(final_kmeans, plot='cluster')

# (Optional) View final clustered data
clustered_data.head()
#k=6(6 optimal number of clusters)

from pycaret.clustering import get_config
from scipy.spatial.distance import pdist, squareform
import seaborn as sns
import matplotlib.pyplot as plt

# üîπ Step 1: Get the transformed data (normalized) from PyCaret
X = get_config("X")

# üîπ Step 2: Compute Euclidean distance matrix
distance_matrix = squareform(pdist(X, metric='euclidean'))

# üîπ Step 3: Plot heatmap (100x100 for visibility)
plt.figure(figsize=(10, 8))
sns.heatmap(distance_matrix[:100, :100], cmap='magma')
plt.title("Distance Matrix Heatmap (KMeans Clustering - 100x100 sample)")
plt.xlabel("Samples")
plt.ylabel("Samples")
plt.show(

"""(ii) Hierarchical clustering"""

import numpy as np
import pandas as pd
from pycaret.clustering import *
from scipy.spatial.distance import pdist, squareform
import seaborn as sns
import matplotlib.pyplot as plt
# üå≥ STEP 5: Hierarchical Clustering
hclust_model = create_model('hclust'

final_hclust = create_model('hclust', num_clusters=4)
hclust_results = assign_model(final_hclust)
plot_model(final_hclust, plot='cluster')

plot_model(hclust_model, plot='elbow')

from pycaret.clustering import get_config
from scipy.spatial.distance import pdist, squareform
import seaborn as sns
import matplotlib.pyplot as plt

# üîπ Step 1: Get the normalized input data used for clustering
X = get_config("X")  # from PyCaret setup()

# üîπ Step 2: Compute the Euclidean distance matrix
distance_matrix = squareform(pdist(X, metric='euclidean'))

# üîπ Step 3: Plot a heatmap (sample 100x100 for clarity)
plt.figure(figsize=(10, 8))
sns.heatmap(distance_matrix[:100, :100], cmap='magma')
plt.title("Distance Matrix Heatmap (100√ó100 sample)")
plt.xlabel("Samples")
plt.ylabel("Samples")
plt.show()

"""Q14. Create a random dataset of 600 rows and 15 columns. All the values are defined between [-100,100]."""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
df=pd.DataFrame(np.random.randint(-100,101,size=(600,15)))
print(df.head())

"""(i) Plot scatter graph between Column 5 and Column 6."""

x=df[5]
y=df[6]
plt.figure(figsize=(8,6))
plt.scatter(x,y,alpha=1,color='pink',edgecolor='blue')      #alpha controls transparency
plt.xlabel('Column 5')
plt.ylabel('Column 6')
plt.title('Scatter Plot of Column 5 vs Column 6')
plt.grid(True)
plt.show()

"""(ii) Plot histogram of each column in single graph."""

plt.figure(figsize=(18,12))
for column in df.columns:
  plt.hist(df[column],bins=20,alpha=0.7,label=column)       # bins refers to intervals or groupings
plt.xlabel('Value')
plt.ylabel('Frequency')
plt.title('Histogram of Each Column')
plt.legend()
plt.tight_layout()

"""(iii) Plot the Box plot of each column in single graph."""

plt.figure(figsize=(18,12))
sns.boxplot(data=df)
plt.xlabel('Column')
plt.ylabel('Value')
plt.title('Box Plot of Each Column')
plt.xticks(rotation=45)
plt.tight_layout()
#plt.show()